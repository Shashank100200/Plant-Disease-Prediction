{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense , Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define paths to training and testing datasets\n",
    "train_path = 'C:/Users/Rohan/Downloads/archive (2)/train'\n",
    "test_path = 'C:/Users/Rohan/Downloads/archive (2)/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_gen = ImageDataGenerator(rescale=(1./255),horizontal_flip=True,shear_range=0.2,zoom_range = 0.2)\n",
    "test_gen = ImageDataGenerator(rescale=(1./255),horizontal_flip=True,shear_range=0.2,zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25848 images belonging to 11 classes.\n",
      "Found 6683 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.flow_from_directory( 'C:/Users/Rohan/Downloads/archive (2)/train',\n",
    "                                      target_size=(120, 120),\n",
    "                                      class_mode='categorical',\n",
    "                                      subset='training',\n",
    "                                      batch_size=9)\n",
    "test = test_gen.flow_from_directory('C:/Users/Rohan/Downloads/archive (2)/valid',\n",
    "                                    target_size=(120, 120),\n",
    "                                      class_mode='categorical',\n",
    "                                      batch_size=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bacterial_spot': 0,\n",
       " 'Early_blight': 1,\n",
       " 'Late_blight': 2,\n",
       " 'Leaf_Mold': 3,\n",
       " 'Septoria_leaf_spot': 4,\n",
       " 'Spider_mites Two-spotted_spider_mite': 5,\n",
       " 'Target_Spot': 6,\n",
       " 'Tomato_Yellow_Leaf_Curl_Virus': 7,\n",
       " 'Tomato_mosaic_virus': 8,\n",
       " 'healthy': 9,\n",
       " 'powdery_mildew': 10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense,BatchNormalization,GlobalAveragePooling2D,Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Block 0\n",
    "model.add(Conv2D(64, (5, 5), strides=1, padding=\"same\", input_shape=(120, 120, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Block 1\n",
    "model.add(Conv2D(64, (5, 5), strides=1, padding=\"same\"))\n",
    "model.add(MaxPooling2D((4, 4)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# Block 2\n",
    "model.add(Conv2D(128, (3, 3), strides=2, padding=\"same\"))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3\n",
    "model.add(Conv2D(256, (7, 7), strides=2, padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.2))  # Adjust dropout rate\n",
    "\n",
    "# Block 4\n",
    "model.add(Conv2D(512, (3, 3), strides=2, padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.25))  # Adjust dropout rate\n",
    "\n",
    "# Block 5\n",
    "model.add(Conv2D(512, (3, 3), strides=2, padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 120, 120, 64)      4864      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 120, 120, 64)      256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 120, 120, 64)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 120, 120, 64)      102464    \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 30, 30, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 30, 30, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 30, 30, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 15, 15, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 7, 7, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 256)         1605888   \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 4, 4, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 2, 2, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 2, 2, 512)         2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 1, 1, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 1, 1, 512)         2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 512)               0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              525312    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 1024)              4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 11)                2827      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6524619 (24.89 MB)\n",
      "Trainable params: 6517963 (24.86 MB)\n",
      "Non-trainable params: 6656 (26.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Global Average Pooling\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))  # Adjust dropout rate\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))  # Adjust dropout rate\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#performing early stopping to avoid overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience = 20, verbose = 1, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create or load your model (ensure final layer has correct units)\n",
    "# model.add(Dense(len(train.class_indices), activation='softmax')) # Output layer with num_classes units# Create or load your model (ensure final layer has correct units)\n",
    "# model.add(Dense(len(train.class_indices), activation='softmax')) # Output layer with num_classes units and softmax activation\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# #performing early stopping to avoid overfitting\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor = 'val_accuracy', mode = 'max', patience = 20, verbose = 1, restore_best_weights = True)\n",
    "\n",
    "# history = model.fit(train,batch_size=10,validation_data=test,epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1021/2872 [=========>....................] - ETA: 29:15 - loss: 2.4507 - accuracy: 0.2282"
     ]
    }
   ],
   "source": [
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossetropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train,batch_size=10,validation_data=test,epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('FinalDraft.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model on the validation generator\n",
    "val_results = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the metrics from the evaluation results\n",
    "val_loss = val_results[0]\n",
    "val_accuracy = val_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation generator\n",
    "val_pred = model.predict(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted probabilities to class labels\n",
    "val_pred_classes = np.argmax(val_pred, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your validation labels are one-hot encoded\n",
    "val_true_classes = test.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(val_true_classes, val_pred_classes)\n",
    "precision = precision_score(val_true_classes, val_pred_classes, average='weighted')\n",
    "recall = recall_score(val_true_classes, val_pred_classes, average='weighted')\n",
    "f1 = f1_score(val_true_classes, val_pred_classes, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(val_true_classes, val_pred_classes)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "\n",
    "\n",
    "# Calculate sensitivity and specificity for each class\n",
    "sensitivity_per_class = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "specificity_per_class = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sensitivity_per_class)):\n",
    "    print(f'Class {i} - Sensitivity (Recall): {sensitivity_per_class[i]}, Specificity: {specificity_per_class[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
